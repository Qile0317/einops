---
title: "Einops tutorial, part 1: basics"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{basics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
)

# figure dimensions
pane_length <- function(p) 0.5 * p
SINGLE_PANE <- "7.142857%"
```

## Welcome to einops-land!

We don't write

```R
y <- aperm(x, c(1, 3, 4, 2))
```

We write comprehensible code

```R
y <- rearrange(x, "b c h w -> b h w c")
```

Currently, `einops` only supports the `base::array`, and extends them.
In the future, more tensor types will be supported such as `torch::torch_tensor`'s.

## What's in this vignette?

- fundamentals: reordering, composition and decomposition of axes
- operations: `rearrange`, `reduce`, `repeat`
- how much you can do with a single operation!

```{r setup}
library(einops)
```

## Load a batch of images to play with

Here we the einops example data, an `image_tensor` object -  a thin wrapper around a 3D and 4D `base::array`. The data 6 images of shape 96x96 with 3 color channels packed into tensor. The main difference with `base::array`'s are that the print method by default will show the image itself.

Note that if the `image_tensor` is indexed with a single value, it is equivalent to indexing a regular array with all other indices empty. So when we see `x[i]` its the same as `x[i, , , ]` or `x[i, , , ,]`.

We also set the option `einops_row_major` to `TRUE`. This is a global option that the package uses because native R arrays are stored in column-major order, which also affects its indexing behaviour to be completely different compared to python's many row-major tensors. Using it here will make the examples much more intuitive, as the images will be indexed by batch, height, width, and color channels, just like in many deep learning frameworks. This option is false by default to preserve the original R behavior, and can be overridden anytime by setting the `.row_major` argument.

```{r load_einops_image}
options(einops_row_major = TRUE)
ims <- get(data("einops_image"))
print(dim(ims))
```

```{r, fig.width = pane_length(1), fig.height = pane_length(1)}
# display the first image (whole 4d tensor can't be rendered)
ims[1]
```

```{r, fig.width = pane_length(1), fig.height = pane_length(1)}
# second image in a batch
ims[2]
```

```{r, fig.width = pane_length(1), fig.height = pane_length(1)}
# rearrange, as the name suggests, rearranges elements
# below we swapped height and width.
# In other words, transposed first two axes (dimensions)
rearrange(ims[1], "h w c -> w h c")
```

```{r, fig.width = pane_length(1), fig.height = pane_length(1)}
# we could use more verbose names for axes, and result is the same:
rearrange(ims[1], "height width color -> width height color")
# when you operate on same set of axes many times,
# you usually come up with short names.
# That's what we do throughout tutorial - we'll use b (for batch), h, w, and c
```

## Composition of axes

Transposition is very common and useful, but let's move to other capabilities provided by einops.

```{r, fig.width = pane_length(1)}
# einops allows seamlessly composing batch and height to a new height dimension
# We just rendered all images by collapsing to 3d tensor!
rearrange(ims, "b h w c -> (b h) w c")
```

```{r fig.height = pane_length(1)}
# or compose a new dimension of batch and width
rearrange(ims, "b h w c -> h (b w) c")
```

```{r}
# resulting dimensions are computed very simply
# length of newly composed axis is a product of components
# [6, 96, 96, 3] -> [96, (6 * 96), 3]
dim(rearrange(ims, "b h w c -> h (b w) c"))
```

```{r}
# we can compose more than two axes.
# let's flatten 4d array into 1d, resulting array has as many elements as the original
dim(rearrange(ims, "b h w c -> (b h w c)"))
```

## Decomposition of axis

```{r}
# decomposition is the inverse process - represent an axis as a combination of new axes
# several decompositions possible, so b1=2 is to decompose 6 to b1=2 and b2=3
dim(rearrange(ims, "(b1 b2) h w c -> b1 b2 h w c ", b1=2))
```

```{r out.width = "28%", fig.height = 2}
# finally, combine composition and decomposition:
rearrange(ims, "(b1 b2) h w c -> (b1 h) (b2 w) c ", b1=2)
```

```{r out.width = "19%", fig.height = 4.5}
# slightly different composition: b1 is merged with width, b2 with height
# ... so letters are ordered by w then by h
rearrange(ims, "(b1 b2) h w c -> (b2 h) (b1 w) c ", b1=2)
```

```{r, fig.height = pane_length(4)}
# move part of width dimension to height.
# we should call this width-to-height as image width shrunk by 2 and height doubled.
# but all pixels are the same!
# Can you write reverse operation (height-to-width)?
rearrange(ims, "b h (w w2) c -> (h w2) (b w) c", w2 = 2)
```

## Order of axes matters

```{r, fig.height = pane_length(1)}
# compare with the next example
rearrange(ims, "b h w c -> h (b w) c")
```

```{r, fig.height = pane_length(1)}
# order of axes in composition is different
# rule is just as for digits in the number: leftmost digit is the most significant,
# while neighboring numbers differ in the rightmost axis.

# you can also think of this as lexicographic sort
rearrange(ims, "b h w c -> h (w b) c")
```

```{r, fig.height = pane_length(1)}
# what if b1 and b2 are reordered before composing to width?
rearrange(ims, "(b1 b2) h w c -> h (b1 b2 w) c ", b1=2)  # produces 'einops'
```

```{r, fig.height = pane_length(1)}
rearrange(ims, "(b1 b2) h w c -> h (b2 b1 w) c ", b1=2)  # produces 'eoipns'
```

## Meet einops.reduce

In einops-land you don't need to guess what happened

```R
apply(x, -ncol(x), mean)
```

if axis is not present in the output — you guessed it — axis was reduced.

```{r fig.width = pane_length(1), fig.height = pane_length(1)}
# average over batch
reduce(ims, "b h w c -> h w c", "mean")
```

```{r fig.width = pane_length(1), fig.height = pane_length(1)}
# the previous is identical to familiar:
as_image_tensor(apply(ims, c(2, 3, 4), mean))
# but is so much more readable
```

```{r fig.width = pane_length(1), fig.height = pane_length(1)}
# Example of reducing of several axes
# besides mean, there are also min, max, sum, prod
reduce(ims, "b h w c -> h w", "min")
```

```{r, fig.height = pane_length(1)}
# this is mean-pooling with 2x2 kernel
# image is split into 2x2 patches, each patch is averaged
reduce(ims, "b (h h2) (w w2) c -> h (b w) c", "mean", h2=2, w2=2)
```

```{r, fig.height = pane_length(1)}
# max-pooling is similar
# result is not as smooth as for mean-pooling
reduce(ims, "b (h h2) (w w2) c -> h (b w) c", "max", h2=2, w2=2)
```

```{r out.width = "19%", fig.height = 4.5}
# yet another example. Can you compute result shape?
reduce(ims, "(b1 b2) h w c -> (b2 h) (b1 w)", "mean", b1=2)
```

## Stack and concatenate

```{r}
# rearrange can also take care of lists of arrays with the same shape
x <- list(ims[1], ims[2], ims[3], ims[4], ims[5], ims[6])
cat("list with", length(x), "tensors of shape", paste(dim(x[[1]]), collapse=" "))
# that's how we can stack inputs
# "list axis" becomes first ("b" in this case), and we left it there
dim(rearrange(x, "b h w c -> b h w c"))
```

```{r}
# but new axis can appear in the other place:
dim(rearrange(x, "b h w c -> h w c b"))
```

```{r}
# that's equivalent to array stacking, but written more explicitly
identical(
    rearrange(x, "b h w c -> h w c b"),
    aperm(array(unlist(x), dim = c(dim(x[[1]]), length(x))), 1:4)
)
```

```{r}
# ... or we can concatenate along axes
dim(rearrange(x, "b h w c -> h (b w) c"))
```

```{r}
# which is equivalent to concatenation
identical(
    rearrange(unclass(x), "b h w c -> h (b w) c"),
    abind::abind(x, along = 2)
)
```
